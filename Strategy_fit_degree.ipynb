{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3528e3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_excel('questions.xlsx',header=None)\n",
    "\n",
    "questions = pd.DataFrame(columns=['Question', 'Colour', 'Concept', 'Discipline', 'Place'])\n",
    "scores = pd.DataFrame(columns=['Question', 'Colour', 'Concept', 'Discipline', 'Place'])\n",
    "\n",
    "preferences = pd.read_csv('perference_transformed.csv')\n",
    "user_preferences = preferences.iloc[1-1].to_dict()\n",
    "\n",
    "decision_datas = pd.read_csv('decision.csv',header=None)\n",
    "decision_data = decision_datas[decision_datas.iloc[:, 0] == 1]\n",
    "\n",
    "groups = {\n",
    "    'Colour': ['PURPLE', 'BLUE', 'GREEN', 'RED', 'ORANGE'],\n",
    "    'Concept': ['RISK', 'HOPE', 'SAFETY', 'VITALITY', 'POWER'],\n",
    "    'Discipline': ['LITERATURE', 'PHYSICS', 'MUSIC', 'HISTORY', 'GEOGRAPHY'],\n",
    "    'Place': ['SEA', 'DESERT', 'CITY', 'MOUNTAIN', 'VILLAGE']\n",
    "}\n",
    "\n",
    "value_to_group = {\n",
    "    value: group\n",
    "    for group, values in groups.items()\n",
    "    for value in values\n",
    "}\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    question_num = row.iloc[0]\n",
    "    groups = re.findall(r'([A-Z]+) - (\\d+), ([A-Z]+) - (\\d+), ([A-Z]+) - (\\d+), ([A-Z]+) - (\\d+)', row.iloc[1])\n",
    "    if groups:\n",
    "        groups = groups[0]\n",
    "        question_row = {\n",
    "            'Question': question_num,\n",
    "            'Colour': groups[0],\n",
    "            'Concept': groups[2],\n",
    "            'Discipline': groups[4],\n",
    "            'Place': groups[6]\n",
    "        }\n",
    "        score_row = {\n",
    "            'Question': question_num,\n",
    "            'Colour': int(groups[1]),\n",
    "            'Concept': int(groups[3]),\n",
    "            'Discipline': int(groups[5]),\n",
    "            'Place': int(groups[7])\n",
    "        }\n",
    "        \n",
    "        questions = pd.concat([questions, pd.DataFrame([question_row])], ignore_index=True)\n",
    "        scores = pd.concat([scores, pd.DataFrame([score_row])], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "281ac672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the options and score of the current question\n",
    "def get_current_options(row, scores, index):\n",
    "    return {\n",
    "        'Colour': (row['Colour'], scores.iloc[index]['Colour']),\n",
    "        'Concept': (row['Concept'], scores.iloc[index]['Concept']),\n",
    "        'Discipline': (row['Discipline'], scores.iloc[index]['Discipline']),\n",
    "        'Place': (row['Place'], scores.iloc[index]['Place'])\n",
    "    }\n",
    "\n",
    "# record the result\n",
    "def record_choice(question_num, best_items, current_options, results):\n",
    "    choices_str = \", \".join(best_items)\n",
    "\n",
    "    scores = []\n",
    "    for item in best_items:\n",
    "        for _, (opt, score) in current_options.items():\n",
    "            if opt == item:\n",
    "                scores.append(str(score))\n",
    "    scores_str = \", \".join(scores)\n",
    "\n",
    "    result_row = {\n",
    "        'Question': question_num,\n",
    "        'Choice': choices_str,\n",
    "        'Score': scores_str\n",
    "    }\n",
    "    results = pd.concat([results, pd.DataFrame([result_row])], ignore_index=True)\n",
    "    return results\n",
    "\n",
    "# Calculate the similarity between the predicted options of the current strategy and the actual options chosen by the subjects\n",
    "# It is the degree of strategy fit\n",
    "# \"decision_choices\" represents the answers given by the subjects, while \"result_choices\" represents the answers predicted by the strategy\n",
    "def calculate_overlap(decision_choices, result_choices):\n",
    "    overlap_count = sum(1 for i in range(len(decision_choices)) if decision_choices[i] in result_choices[i])\n",
    "    overlap_rate = overlap_count / len(decision_choices)\n",
    "    return overlap_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73423fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-RL (Q-Reinforcement Learning)\n",
    "# It is not a strict form of Q-learning, but merely uses the framework.\n",
    "# Similar to the forgetting mechanism, the older the score, the lower its weight.\n",
    "def evaluate_q_rl(decision_data: pd.DataFrame) -> tuple:\n",
    "    \n",
    "    results = pd.DataFrame(columns=['Question', 'Choice', 'Score'])\n",
    "    learning_rate = 0.1\n",
    "    discount_factor = 0.9\n",
    "\n",
    "    # Initialize the Q-values of all possible options to 0.\n",
    "    all_options = set()\n",
    "    for _, row in questions.iterrows():\n",
    "        all_options.update([\n",
    "            row['Colour'], row['Concept'], \n",
    "            row['Discipline'], row['Place']\n",
    "        ])\n",
    "    q_values = {option: 0 for option in all_options}\n",
    "\n",
    "    for index, row in questions.iterrows():\n",
    "        current_options = get_current_options(row, scores, index)\n",
    "\n",
    "        # Find all the options with the highest Q values\n",
    "        options_with_q = [(option, q_values[option]) for option, _ in current_options.values()]\n",
    "        max_q = max(q for _, q in options_with_q)\n",
    "        best_items = [option for option, q in options_with_q if q == max_q]\n",
    "        \n",
    "        # Update the Q values of all options\n",
    "        for option, current_score in current_options.values():\n",
    "            old_q = q_values[option]\n",
    "            max_future_q = max(q_values[opt] for opt, _ in current_options.values())\n",
    "            new_q = old_q + learning_rate * (current_score + discount_factor * max_future_q - old_q)\n",
    "            q_values[option] = new_q\n",
    "        \n",
    "        results = record_choice(row['Question'], best_items, current_options, results)\n",
    "\n",
    "    overlap_rate = calculate_overlap(decision_data.iloc[:, 2].tolist(), results['Choice'].tolist())\n",
    "    first5_overlap_rate = calculate_overlap(decision_data.iloc[:5, 2].tolist(), results['Choice'][:5].tolist())\n",
    "    mid15_overlap_rate = calculate_overlap(decision_data.iloc[5:20, 2].tolist(), results['Choice'][5:20].tolist())\n",
    "    last20_overlap_rate = calculate_overlap(decision_data.iloc[20:, 2].tolist(), results['Choice'][20:].tolist())\n",
    "    \n",
    "    return (f\"{overlap_rate:.2%}\", f\"{first5_overlap_rate:.2%}\", f\"{mid15_overlap_rate:.2%}\", f\"{last20_overlap_rate:.2%}\")\n",
    "evaluate_q_rl(decision_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "690d6c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R-Item (Item Recency)\n",
    "# Make the selection directly based on the score of the previous round's options\n",
    "\n",
    "def evaluate_R_Item(decision_data: pd.DataFrame) -> tuple:\n",
    "\n",
    "    results = pd.DataFrame(columns=['Question', 'Choice', 'Score'])\n",
    "    history = {}\n",
    "\n",
    "    for index, row in questions.iterrows():\n",
    "        current_options = get_current_options(row, scores, index)\n",
    "\n",
    "        # If history is empty, list all the options\n",
    "        if not any(option in history for option, _ in current_options.values()):\n",
    "            best_items = [option for option, _ in current_options.values()]\n",
    "        else:\n",
    "            options_with_history = []\n",
    "            for option, current_score in current_options.values():\n",
    "                if option in history:\n",
    "                    options_with_history.append((option, history[option]))\n",
    "            # Select the option for obtaining the highest score from the previous round\n",
    "            max_score = max(score for _, score in options_with_history)\n",
    "            best_items = [option for option, score in options_with_history if score == max_score]\n",
    "\n",
    "        # Update History\n",
    "        for option, current_score in current_options.values():\n",
    "            history[option] = current_score\n",
    "\n",
    "        results = record_choice(row['Question'], best_items, current_options, results)\n",
    "\n",
    "    overlap_rate = calculate_overlap(decision_data.iloc[:, 2].tolist(), results['Choice'].tolist())\n",
    "    first5_overlap_rate = calculate_overlap(decision_data.iloc[:5, 2].tolist(), results['Choice'][:5].tolist())\n",
    "    mid15_overlap_rate = calculate_overlap(decision_data.iloc[5:20, 2].tolist(), results['Choice'][5:20].tolist())\n",
    "    last20_overlap_rate = calculate_overlap(decision_data.iloc[20:, 2].tolist(), results['Choice'][20:].tolist())\n",
    "    \n",
    "    return (f\"{overlap_rate:.2%}\", f\"{first5_overlap_rate:.2%}\", f\"{mid15_overlap_rate:.2%}\", f\"{last20_overlap_rate:.2%}\")\n",
    "evaluate_R_Item(decision_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a523d381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R-Cat (Category Recency)\n",
    "# Make your choice directly based on the group scores from the previous question\n",
    "\n",
    "def evaluate_Simple_R_Cat(decision_data: pd.DataFrame) -> tuple:\n",
    "    results = pd.DataFrame(columns=['Question', 'Choice', 'Score'])\n",
    "    history = {}  # {category: last_score}\n",
    "\n",
    "    for index, row in questions.iterrows():\n",
    "        current_options = get_current_options(row, scores, index)\n",
    "\n",
    "        # If the category history is empty, select all categories\n",
    "        if not any(category in history for category in current_options.keys()):\n",
    "            best_categories = list(current_options.keys())\n",
    "        else:\n",
    "            categories_with_history = []\n",
    "            for category, (option, current_score) in current_options.items():\n",
    "                if category in history:\n",
    "                    categories_with_history.append((category, history[category]))\n",
    "\n",
    "            max_score = max(score for _, score in categories_with_history)\n",
    "            best_categories = [category for category, score in categories_with_history if score == max_score]\n",
    "\n",
    "        # Obtain all possible items based on all possible categories\n",
    "        best_items = [current_options[cat][0] for cat in best_categories]\n",
    "\n",
    "        # Update History\n",
    "        for category, (option, current_score) in current_options.items():\n",
    "            history[category] = current_score\n",
    "\n",
    "        results = record_choice(row['Question'], best_items, current_options, results)\n",
    "\n",
    "    overlap_rate = calculate_overlap(decision_data.iloc[:, 2].tolist(), results['Choice'].tolist())\n",
    "    first5_overlap_rate = calculate_overlap(decision_data.iloc[:5, 2].tolist(), results['Choice'][:5].tolist())\n",
    "    mid15_overlap_rate = calculate_overlap(decision_data.iloc[5:20, 2].tolist(), results['Choice'][5:20].tolist())\n",
    "    last20_overlap_rate = calculate_overlap(decision_data.iloc[20:, 2].tolist(), results['Choice'][20:].tolist())\n",
    "    \n",
    "    return (f\"{overlap_rate:.2%}\", f\"{first5_overlap_rate:.2%}\", f\"{mid15_overlap_rate:.2%}\", f\"{last20_overlap_rate:.2%}\")\n",
    "evaluate_Simple_R_Cat(decision_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33d1605c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSC (Least Selected Category)\n",
    "# Choose the group with the least number of previous selections\n",
    "\n",
    "def evaluate_LSC(decision_data: pd.DataFrame) -> tuple:\n",
    "    results = pd.DataFrame(columns=['Question', 'Choice', 'Score'])\n",
    "    selection_count = {\n",
    "    'Colour': 0,\n",
    "    'Concept': 0,\n",
    "    'Discipline': 0,\n",
    "    'Place': 0\n",
    "    }\n",
    "    \n",
    "    for index, row in questions.iterrows():\n",
    "        current_options = get_current_options(row, scores, index)\n",
    "\n",
    "        min_count = min(selection_count.values())\n",
    "        least_selected = [category for category in current_options.keys() \n",
    "                        if selection_count[category] == min_count]\n",
    "        best_items = [current_options[option][0] for option in least_selected]\n",
    "\n",
    "        # Update History\n",
    "        user_decision_item = decision_data.iloc[index, 2]\n",
    "        selection_count[value_to_group[user_decision_item]] += 1\n",
    "\n",
    "        results = record_choice(row['Question'], best_items, current_options, results)\n",
    "\n",
    "    overlap_rate = calculate_overlap(decision_data.iloc[:, 2].tolist(), results['Choice'].tolist())\n",
    "    first5_overlap_rate = calculate_overlap(decision_data.iloc[:5, 2].tolist(), results['Choice'][:5].tolist())\n",
    "    mid15_overlap_rate = calculate_overlap(decision_data.iloc[5:20, 2].tolist(), results['Choice'][5:20].tolist())\n",
    "    last20_overlap_rate = calculate_overlap(decision_data.iloc[20:, 2].tolist(), results['Choice'][20:].tolist())\n",
    "    \n",
    "    return (f\"{overlap_rate:.2%}\", f\"{first5_overlap_rate:.2%}\", f\"{mid15_overlap_rate:.2%}\", f\"{last20_overlap_rate:.2%}\")\n",
    "evaluate_LSC(decision_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f97dc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WSLS (Win-Stay, Lose-Shift)\n",
    "# If the current option category score is greater than or equal to three, \n",
    "# then continue to select. \n",
    "# On the contrary, you can choose any other category.\n",
    "\n",
    "def evaluate_WSLS(decision_data: pd.DataFrame) -> tuple:\n",
    "    results = pd.DataFrame(columns=['Question', 'Choice', 'Score'])\n",
    "    valid_groups = ['Colour', 'Concept', 'Discipline', 'Place']\n",
    "    best_category = valid_groups\n",
    "\n",
    "    for index, row in questions.iterrows():\n",
    "        current_options = get_current_options(row, scores, index)\n",
    "        user_decision_groups = value_to_group[decision_data.iloc[index, 2]]\n",
    "\n",
    "        best_items = [current_options[option][0] for option in best_category]\n",
    "        results = record_choice(row['Question'], best_items, current_options, results)\n",
    "\n",
    "        if current_options[user_decision_groups][1] < 3:\n",
    "            best_category = [g for g in valid_groups if g != user_decision_groups]\n",
    "        else:\n",
    "            best_category = [user_decision_groups]\n",
    "\n",
    "    overlap_rate = calculate_overlap(decision_data.iloc[:, 2].tolist(), results['Choice'].tolist())\n",
    "    first5_overlap_rate = calculate_overlap(decision_data.iloc[:5, 2].tolist(), results['Choice'][:5].tolist())\n",
    "    mid15_overlap_rate = calculate_overlap(decision_data.iloc[5:20, 2].tolist(), results['Choice'][5:20].tolist())\n",
    "    last20_overlap_rate = calculate_overlap(decision_data.iloc[20:, 2].tolist(), results['Choice'][20:].tolist())\n",
    "\n",
    "    return (f\"{overlap_rate:.2%}\", f\"{first5_overlap_rate:.2%}\", f\"{mid15_overlap_rate:.2%}\", f\"{last20_overlap_rate:.2%}\")\n",
    "evaluate_WSLS(decision_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99391482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EV (Expected Value)\n",
    "# Choose the option with the highest average expected value\n",
    "\n",
    "def evaluate_EV(decision_data: pd.DataFrame) -> tuple:\n",
    "    results = pd.DataFrame(columns=['Question', 'Choice', 'Score'])\n",
    "    expectations = {}  # {option: (total, count)}\n",
    "\n",
    "    for index, row in questions.iterrows():\n",
    "        current_options = get_current_options(row, scores, index)\n",
    "\n",
    "        options_with_expectation = []\n",
    "        for option, current_score in current_options.values():\n",
    "            if option in expectations:\n",
    "                total, count = expectations[option]\n",
    "                avg_score = total / count\n",
    "            else:\n",
    "                avg_score = 0\n",
    "            options_with_expectation.append((option, avg_score))\n",
    "\n",
    "        max_avg = max(avg for _, avg in options_with_expectation)\n",
    "        best_items = [option for option, avg in options_with_expectation if avg == max_avg]\n",
    "\n",
    "        for option, current_score in current_options.values():\n",
    "            if option in expectations:\n",
    "                total, count = expectations[option]\n",
    "                expectations[option] = (total + current_score, count + 1)\n",
    "            else:\n",
    "                expectations[option] = (current_score, 1)\n",
    "\n",
    "        results = record_choice(row['Question'], best_items, current_options, results)\n",
    "\n",
    "    overlap_rate = calculate_overlap(decision_data.iloc[:, 2].tolist(), results['Choice'].tolist())\n",
    "    first5_overlap_rate = calculate_overlap(decision_data.iloc[:5, 2].tolist(), results['Choice'][:5].tolist())\n",
    "    mid15_overlap_rate = calculate_overlap(decision_data.iloc[5:20, 2].tolist(), results['Choice'][5:20].tolist())\n",
    "    last20_overlap_rate = calculate_overlap(decision_data.iloc[20:, 2].tolist(), results['Choice'][20:].tolist())\n",
    "    \n",
    "    return (f\"{overlap_rate:.2%}\", f\"{first5_overlap_rate:.2%}\", f\"{mid15_overlap_rate:.2%}\", f\"{last20_overlap_rate:.2%}\")\n",
    "evaluate_EV(decision_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c7bf197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RA (Risk-Averse)\n",
    "# Select the option with the highest lower limit (mean-std)\n",
    "\n",
    "def evaluate_RA(decision_data: pd.DataFrame) -> tuple:\n",
    "\n",
    "    results = pd.DataFrame(columns=['Question', 'Choice', 'Score'])\n",
    "    score_history = {}  # {option: [scores]}\n",
    "\n",
    "    for index, row in questions.iterrows():\n",
    "        current_options = get_current_options(row, scores, index)\n",
    "\n",
    "        options_with_variance = []\n",
    "        for option, current_score in current_options.values():\n",
    "            if option in score_history and len(score_history[option]) > 1:\n",
    "                s = score_history[option]\n",
    "                mean = sum(s) / len(s)\n",
    "                std = np.sqrt(sum((x - mean) ** 2 for x in s) / len(s))\n",
    "                score = mean - std\n",
    "            else:\n",
    "                score = 0\n",
    "            options_with_variance.append((option, score))\n",
    "\n",
    "        min_var = max(var for _, var in options_with_variance)\n",
    "        best_items = [option for option, var in options_with_variance if var == min_var]\n",
    "\n",
    "        for option, current_score in current_options.values():\n",
    "            score_history.setdefault(option, []).append(current_score)\n",
    "\n",
    "        results = record_choice(row['Question'], best_items, current_options, results)\n",
    "        \n",
    "    overlap_rate = calculate_overlap(decision_data.iloc[:, 2].tolist(), results['Choice'].tolist())\n",
    "    first5_overlap_rate = calculate_overlap(decision_data.iloc[:5, 2].tolist(), results['Choice'][:5].tolist())\n",
    "    mid15_overlap_rate = calculate_overlap(decision_data.iloc[5:20, 2].tolist(), results['Choice'][5:20].tolist())\n",
    "    last20_overlap_rate = calculate_overlap(decision_data.iloc[20:, 2].tolist(), results['Choice'][20:].tolist())\n",
    "    \n",
    "    return (f\"{overlap_rate:.2%}\", f\"{first5_overlap_rate:.2%}\", f\"{mid15_overlap_rate:.2%}\", f\"{last20_overlap_rate:.2%}\")\n",
    "evaluate_RA(decision_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23ba5d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RS (Risk-Seeking)\n",
    "# Select the option with the highest upper limit (mean + std)\n",
    "\n",
    "def evaluate_RS(decision_data: pd.DataFrame) -> tuple:\n",
    "\n",
    "    results = pd.DataFrame(columns=['Question', 'Choice', 'Score'])\n",
    "    score_history = {}  # {option: [scores]}\n",
    "\n",
    "    for index, row in questions.iterrows():\n",
    "        current_options = get_current_options(row, scores, index)\n",
    "\n",
    "        options_with_variance = []\n",
    "        for option, current_score in current_options.values():\n",
    "            if option in score_history and len(score_history[option]) > 1:\n",
    "                s = score_history[option]\n",
    "                mean = sum(s) / len(s)\n",
    "                std = np.sqrt(sum((x - mean) ** 2 for x in s) / len(s))\n",
    "                score = mean + std\n",
    "            else:\n",
    "                score = 0\n",
    "            options_with_variance.append((option, score))\n",
    "\n",
    "        max_var = max(var for _, var in options_with_variance)\n",
    "        best_items = [option for option, var in options_with_variance if var == max_var]\n",
    "\n",
    "        for option, current_score in current_options.values():\n",
    "            score_history.setdefault(option, []).append(current_score)\n",
    "\n",
    "        results = record_choice(row['Question'], best_items, current_options, results)\n",
    "\n",
    "    overlap_rate = calculate_overlap(decision_data.iloc[:, 2].tolist(), results['Choice'].tolist())\n",
    "    first5_overlap_rate = calculate_overlap(decision_data.iloc[:5, 2].tolist(), results['Choice'][:5].tolist())\n",
    "    mid15_overlap_rate = calculate_overlap(decision_data.iloc[5:20, 2].tolist(), results['Choice'][5:20].tolist())\n",
    "    last20_overlap_rate = calculate_overlap(decision_data.iloc[20:, 2].tolist(), results['Choice'][20:].tolist())\n",
    "    \n",
    "    return (f\"{overlap_rate:.2%}\", f\"{first5_overlap_rate:.2%}\", f\"{mid15_overlap_rate:.2%}\", f\"{last20_overlap_rate:.2%}\")\n",
    "evaluate_RS(decision_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20a2046c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PD (Preference-Driven)\n",
    "# Select the option with the highest preference value.\n",
    "\n",
    "def evaluate_Preference(decision_data: pd.DataFrame, user_preferences: pd.DataFrame) -> tuple:\n",
    "\n",
    "    score_history = {}\n",
    "    results = pd.DataFrame(columns=['Question', 'Choice', 'Score'])\n",
    "\n",
    "    for index, row in questions.iterrows():\n",
    "        current_options = get_current_options(row, scores, index)\n",
    "        \n",
    "        # Construct the preference matrix for the current problem\n",
    "        preference_matrix = []\n",
    "        for option, current_score in current_options.values():\n",
    "            pref_score = user_preferences.get(option)\n",
    "            preference_matrix.append((option, pref_score))\n",
    "        \n",
    "        # Select the highest preference value\n",
    "        max_pref = max(pref for _, pref in preference_matrix)\n",
    "        best_options = [option for option, pref in preference_matrix if pref == max_pref]\n",
    "        for option, current_score in current_options.values():\n",
    "            score_history.setdefault(option, []).append(current_score)\n",
    "        results = record_choice(row['Question'], best_options, current_options, results)\n",
    "        \n",
    "    overlap_rate = calculate_overlap(decision_data.iloc[:, 2].tolist(), results['Choice'].tolist())\n",
    "    first5_overlap_rate = calculate_overlap(decision_data.iloc[:5, 2].tolist(), results['Choice'][:5].tolist())\n",
    "    mid15_overlap_rate = calculate_overlap(decision_data.iloc[5:20, 2].tolist(), results['Choice'][5:20].tolist())\n",
    "    last20_overlap_rate = calculate_overlap(decision_data.iloc[20:, 2].tolist(), results['Choice'][20:].tolist())\n",
    "    \n",
    "    return (f\"{overlap_rate:.2%}\", f\"{first5_overlap_rate:.2%}\", f\"{mid15_overlap_rate:.2%}\", f\"{last20_overlap_rate:.2%}\")\n",
    "evaluate_Preference(decision_data, user_preferences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ec5201a",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies = [\n",
    "    (\"Q-RL\", evaluate_q_rl),\n",
    "    (\"R-Item\", evaluate_R_Item),\n",
    "    (\"R-Cat\", evaluate_Simple_R_Cat),\n",
    "    (\"LSC\", evaluate_LSC),\n",
    "    (\"WSLS\", evaluate_WSLS),\n",
    "    (\"EV\", evaluate_EV),\n",
    "    (\"RA\", evaluate_RA),\n",
    "    (\"RS\", evaluate_RS),\n",
    "    (\"PD\", evaluate_Preference)  # include user_preferences\n",
    "]\n",
    "\n",
    "# Initialization\n",
    "cols = []\n",
    "for name, _ in strategies:\n",
    "    for suffix in [\"_all\", \"_first5\", \"_mid15\", \"_last20\"]:\n",
    "        cols.append(name + suffix)\n",
    "\n",
    "df_out = pd.DataFrame(index=range(1, 54), columns=cols)\n",
    "\n",
    "for i in range(1, 54):\n",
    "    decision_data = decision_datas[decision_datas.iloc[:, 0] == i]\n",
    "\n",
    "    for name, func in strategies:\n",
    "        if name == \"PD\":\n",
    "            user_preferences = preferences.iloc[i-1].to_dict()\n",
    "            res = func(decision_data, user_preferences)\n",
    "        else:\n",
    "            res = func(decision_data)\n",
    "\n",
    "        p_all, p_first5, p_mid15, p_last20 = res\n",
    "        df_out.loc[i, f\"{name}_all\"] = p_all\n",
    "        df_out.loc[i, f\"{name}_first5\"] = p_first5\n",
    "        df_out.loc[i, f\"{name}_mid15\"] = p_mid15\n",
    "        df_out.loc[i, f\"{name}_last20\"] = p_last20\n",
    "\n",
    "# Save as CSV\n",
    "df_out.to_csv(\"strategy_scores.csv\", index_label=\"participant_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a6678f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"strategy_scores.csv\", index_col=\"participant_id\")\n",
    "\n",
    "# The strategy sequence (in line with the order of CSV columns) is saved as a CSV file\n",
    "strategies_order = [\"Q-RL\", \"R-Item\", \"R-Cat\", \"LSC\", \"WSLS\", \"EV\", \"RA\", \"RS\", \"PD\"]\n",
    "suffixes = [\"all\", \"first5\", \"mid15\", \"last20\"]\n",
    "\n",
    "# The strategies to be deleted in the \"first5\" category\n",
    "# Because these strategies were not sufficient to make a choice based on the information provided in the first five questions\n",
    "# For example, there is a lack of standard deviation or the word only appears once\n",
    "to_remove = {\"EV\", \"RA\", \"RS\", \"Q-RL\", \"R-Item\"}\n",
    "\n",
    "for pid in range(1, 54):\n",
    "\n",
    "    row = df.loc[pid]\n",
    "    score_sets = {}\n",
    "\n",
    "    # Read and convert the percentage to a floating-point number\n",
    "    for suf in suffixes:\n",
    "        cols = [f\"{st}_{suf}\" for st in strategies_order]\n",
    "        raw_vals = row[cols].astype(str)\n",
    "        float_vals = raw_vals.str.replace(\"%\", \"\", regex=False).astype(float) / 100.0\n",
    "        score_sets[suf] = float_vals.values\n",
    "\n",
    "    # First 5 Strategy Modification (Delete 5 Strategies)\n",
    "    strategies_first5 = [s for s in strategies_order if s not in to_remove]\n",
    "    values_first5 = [score_sets[\"first5\"][strategies_order.index(s)] for s in strategies_first5]\n",
    "\n",
    "    # Angle (radar chart)\n",
    "    angles_all = np.linspace(0, 2 * np.pi, len(strategies_order), endpoint=False).tolist()\n",
    "    angles_first5 = np.linspace(0, 2 * np.pi, len(strategies_first5), endpoint=False).tolist()\n",
    "\n",
    "    # Plotting\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10), subplot_kw=dict(polar=True))\n",
    "    axes = axes.flatten()\n",
    "    titles = [\"All Trials\", \"First 5 Trials\", \"Middle 15 Trials\", \"Last 20 Trials\"]\n",
    "\n",
    "    for ax, suf, title in zip(axes, suffixes, titles):\n",
    "\n",
    "        if suf == \"first5\":\n",
    "            values = values_first5[:]\n",
    "            strategies = strategies_first5\n",
    "            ang = angles_first5[:]\n",
    "        else:\n",
    "            values = list(score_sets[suf])\n",
    "            strategies = strategies_order\n",
    "            ang = angles_all[:]\n",
    "\n",
    "        values += values[:1]\n",
    "        ang = ang + ang[:1]\n",
    "\n",
    "        ax.plot(ang, values, linewidth=2)\n",
    "        ax.fill(ang, values, alpha=0.25)\n",
    "\n",
    "        ax.set_xticks(ang[:-1])\n",
    "        ax.set_xticklabels(strategies, fontsize=9)\n",
    "\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_title(title, fontsize=13)\n",
    "\n",
    "    fig.suptitle(f\"Participant {pid} â€” Strategy Fit Radar Charts\", fontsize=16, y=1.02)\n",
    "    plt.savefig(f\"strategy_fit_degree/participant_{pid}.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1a77253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "999bd125",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('questions.xlsx',header=None)\n",
    "\n",
    "questions = pd.DataFrame(columns=['Question', 'Colour', 'Concept', 'Discipline', 'Place'])\n",
    "scores = pd.DataFrame(columns=['Question', 'Colour', 'Concept', 'Discipline', 'Place'])\n",
    "\n",
    "preferences = pd.read_csv('perference_transformed.csv')\n",
    "user_preferences = preferences.iloc[1-1].to_dict()\n",
    "\n",
    "decision_datas = pd.read_csv('decision.csv',header=None)\n",
    "decision_data = decision_datas[decision_datas.iloc[:, 0] == 1]\n",
    "\n",
    "groups = {\n",
    "    'Colour': ['PURPLE', 'BLUE', 'GREEN', 'RED', 'ORANGE'],\n",
    "    'Concept': ['RISK', 'HOPE', 'SAFETY', 'VITALITY', 'POWER'],\n",
    "    'Discipline': ['LITERATURE', 'PHYSICS', 'MUSIC', 'HISTORY', 'GEOGRAPHY'],\n",
    "    'Place': ['SEA', 'DESERT', 'CITY', 'MOUNTAIN', 'VILLAGE']\n",
    "}\n",
    "\n",
    "value_to_group = {\n",
    "    value: group\n",
    "    for group, values in groups.items()\n",
    "    for value in values\n",
    "}\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    question_num = row.iloc[0]\n",
    "    groups = re.findall(r'([A-Z]+) - (\\d+), ([A-Z]+) - (\\d+), ([A-Z]+) - (\\d+), ([A-Z]+) - (\\d+)', row.iloc[1])\n",
    "    if groups:\n",
    "        groups = groups[0]\n",
    "        question_row = {\n",
    "            'Question': question_num,\n",
    "            'Colour': groups[0],\n",
    "            'Concept': groups[2],\n",
    "            'Discipline': groups[4],\n",
    "            'Place': groups[6]\n",
    "        }\n",
    "        score_row = {\n",
    "            'Question': question_num,\n",
    "            'Colour': int(groups[1]),\n",
    "            'Concept': int(groups[3]),\n",
    "            'Discipline': int(groups[5]),\n",
    "            'Place': int(groups[7])\n",
    "        }\n",
    "        \n",
    "        questions = pd.concat([questions, pd.DataFrame([question_row])], ignore_index=True)\n",
    "        scores = pd.concat([scores, pd.DataFrame([score_row])], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54aeadab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the options and score of the current question\n",
    "def get_current_options(row, scores, index):\n",
    "    return {\n",
    "        'Colour': (row['Colour'], scores.iloc[index]['Colour']),\n",
    "        'Concept': (row['Concept'], scores.iloc[index]['Concept']),\n",
    "        'Discipline': (row['Discipline'], scores.iloc[index]['Discipline']),\n",
    "        'Place': (row['Place'], scores.iloc[index]['Place'])\n",
    "    }\n",
    "\n",
    "# record the result\n",
    "def record_choice(question_num, best_items, current_options, results):\n",
    "    choices_str = \", \".join(best_items)\n",
    "\n",
    "    scores = []\n",
    "    for item in best_items:\n",
    "        for _, (opt, score) in current_options.items():\n",
    "            if opt == item:\n",
    "                scores.append(str(score))\n",
    "    scores_str = \", \".join(scores)\n",
    "\n",
    "    result_row = {\n",
    "        'Question': question_num,\n",
    "        'Choice': choices_str,\n",
    "        'Score': scores_str\n",
    "    }\n",
    "    results = pd.concat([results, pd.DataFrame([result_row])], ignore_index=True)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58d88241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-RL\n",
    "# Q-Reinforcement Learning\n",
    "results = pd.DataFrame(columns=['Question', 'Choice', 'Score'])\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.9\n",
    "\n",
    "# initialize Q values\n",
    "all_options = set()\n",
    "for _, row in questions.iterrows():\n",
    "    all_options.update([row['Colour'], row['Concept'], row['Discipline'], row['Place']])\n",
    "q_values = {opt: 0 for opt in all_options}\n",
    "\n",
    "# Q-Learning main loop\n",
    "for index, row in questions.iterrows():\n",
    "    current_options = get_current_options(row, scores, index)\n",
    "\n",
    "    # locate highest Q options (may be multiple)\n",
    "    options_with_q = [(opt, q_values[opt]) for opt, _ in current_options.values()]\n",
    "    max_q = max(q for _, q in options_with_q)\n",
    "    best_items = [opt for opt, q in options_with_q if q == max_q]\n",
    "\n",
    "    # update Q values\n",
    "    for opt, s in current_options.values():\n",
    "        old_q = q_values[opt]\n",
    "        max_future = max(q_values[o] for o, _ in current_options.values())\n",
    "        q_values[opt] = old_q + learning_rate * (s + discount_factor * max_future - old_q)\n",
    "\n",
    "    # record results\n",
    "    results = record_choice(row['Question'], best_items, current_options, results)\n",
    "\n",
    "# ensure Score column is comma-separated string\n",
    "results[\"Score\"] = [\",\".join(str(x) for x in sc) if isinstance(sc, list) else str(sc) for sc in results[\"Score\"]]\n",
    "\n",
    "# compute cumulative running average score\n",
    "running_avg = []\n",
    "sum_scores, total_cnt = 0, 0\n",
    "for s in results[\"Score\"]:\n",
    "    values = [float(x) for x in s.split(\",\")]\n",
    "    for v in values:\n",
    "        sum_scores += v\n",
    "        total_cnt += 1\n",
    "    running_avg.append(sum_scores / total_cnt)\n",
    "results[\"RunningAvgScore\"] = running_avg\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(results[\"Question\"], results[\"RunningAvgScore\"], marker='o')\n",
    "plt.ylim(2.2, 2.8)\n",
    "plt.title(\"Average Score Across Questions (Q-RL)\")\n",
    "plt.xlabel(\"Question Number\")\n",
    "plt.ylabel(\"Cumulative Mean Score\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"average_score/Q_RL_avg_score.png\", dpi=400)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ed095fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['Question', 'Choice', 'Score'])\n",
    "history = {}\n",
    "\n",
    "for index, row in questions.iterrows():\n",
    "    current_options = get_current_options(row, scores, index)\n",
    "\n",
    "    # choose best items according to most recent score history\n",
    "    if not any(opt in history for opt, _ in current_options.values()):\n",
    "        best_items = [opt for opt, _ in current_options.values()]\n",
    "    else:\n",
    "        items_with_scores = [(opt, history[opt]) for opt, _ in current_options.values() if opt in history]\n",
    "        max_score = max(sc for _, sc in items_with_scores)\n",
    "        best_items = [opt for opt, sc in items_with_scores if sc == max_score]\n",
    "\n",
    "    # update recency memory\n",
    "    for opt, sc in current_options.values():\n",
    "        history[opt] = sc\n",
    "\n",
    "    # record selection + scores\n",
    "    results = record_choice(row['Question'], best_items, current_options, results)\n",
    "\n",
    "# convert Score to comma-separated string\n",
    "results[\"Score\"] = [\n",
    "    \",\".join(str(x) for x in sc) if isinstance(sc, list) else str(sc)\n",
    "    for sc in results[\"Score\"]\n",
    "]\n",
    "\n",
    "# compute cumulative running mean score\n",
    "running_avg = []\n",
    "total, count = 0, 0\n",
    "for s in results[\"Score\"]:\n",
    "    values = [float(x) for x in s.split(\",\")]\n",
    "    for v in values:\n",
    "        total += v\n",
    "        count += 1\n",
    "    running_avg.append(total / count)\n",
    "\n",
    "results[\"RunningAvgScore\"] = running_avg\n",
    "\n",
    "# plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(results[\"Question\"], results[\"RunningAvgScore\"], marker='o')\n",
    "plt.ylim(2.2, 2.8)\n",
    "plt.title(\"Average Score Over Questions (R-Item)\")\n",
    "plt.xlabel(\"Question Number\")\n",
    "plt.ylabel(\"Cumulative Mean Score\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"average_score/R_Item_avg_score.png\", dpi=400)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "676833af",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['Question', 'Choice', 'Score'])\n",
    "history = {}   # {category: last_score}\n",
    "\n",
    "# main loop\n",
    "for index, row in questions.iterrows():\n",
    "    current_options = get_current_options(row, scores, index)\n",
    "\n",
    "    # choose categories with highest last score\n",
    "    if not any(cat in history for cat in current_options.keys()):\n",
    "        best_categories = list(current_options.keys())\n",
    "    else:\n",
    "        cands = [(cat, history[cat]) for cat in current_options.keys() if cat in history]\n",
    "        max_score = max(sc for _, sc in cands)\n",
    "        best_categories = [cat for cat, sc in cands if sc == max_score]\n",
    "\n",
    "    # translate categories → selected items\n",
    "    best_items = [current_options[cat][0] for cat in best_categories]\n",
    "\n",
    "    # update recency score memory\n",
    "    for cat, (item, score) in current_options.items():\n",
    "        history[cat] = score\n",
    "\n",
    "    results = record_choice(row['Question'], best_items, current_options, results)\n",
    "\n",
    "# convert Score to comma-separated string\n",
    "results[\"Score\"] = [\n",
    "    \",\".join(str(x) for x in sc) if isinstance(sc, list) else str(sc)\n",
    "    for sc in results[\"Score\"]\n",
    "]\n",
    "\n",
    "# compute cumulative running mean score up to each question\n",
    "running_avg = []\n",
    "s, n = 0, 0\n",
    "for score_str in results[\"Score\"]:\n",
    "    values = [float(x) for x in score_str.split(\",\")]\n",
    "    for v in values:\n",
    "        s += v\n",
    "        n += 1\n",
    "    running_avg.append(s / n)\n",
    "\n",
    "results[\"RunningAvgScore\"] = running_avg\n",
    "\n",
    "# plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(results[\"Question\"], results[\"RunningAvgScore\"], marker='o')\n",
    "plt.ylim(2.2, 2.8)\n",
    "plt.title(\"Average Score Over Questions (R-Cat)\")\n",
    "plt.xlabel(\"Question Number\")\n",
    "plt.ylabel(\"Cumulative Mean Score\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"average_score/R_Cat_avg_score.png\", dpi=400)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "077b9800",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['Question', 'Choice', 'Score'])\n",
    "expectations = {}   # {option: (total_score_sum, count)}\n",
    "\n",
    "# main loop\n",
    "for index, row in questions.iterrows():\n",
    "    current_options = get_current_options(row, scores, index)\n",
    "\n",
    "    # compute expectation for current available options\n",
    "    exp_list = []\n",
    "    for option, current_score in current_options.values():\n",
    "        if option in expectations:\n",
    "            total, cnt = expectations[option]\n",
    "            avg = total / cnt\n",
    "        else:\n",
    "            avg = 0\n",
    "        exp_list.append((option, avg))\n",
    "\n",
    "    # find best items (may have multiple)\n",
    "    max_avg = max(v for _, v in exp_list)\n",
    "    best_items = [opt for opt, v in exp_list if v == max_avg]\n",
    "\n",
    "    # update expectation memory\n",
    "    for option, current_score in current_options.values():\n",
    "        if option in expectations:\n",
    "            total, cnt = expectations[option]\n",
    "            expectations[option] = (total + current_score, cnt + 1)\n",
    "        else:\n",
    "            expectations[option] = (current_score, 1)\n",
    "\n",
    "    results = record_choice(row['Question'], best_items, current_options, results)\n",
    "\n",
    "# convert Score to comma-separated string\n",
    "results[\"Score\"] = [\n",
    "    \",\".join(str(x) for x in sc) if isinstance(sc, list) else str(sc)\n",
    "    for sc in results[\"Score\"]\n",
    "]\n",
    "\n",
    "# compute cumulative running mean score\n",
    "running_avg = []\n",
    "s, n = 0, 0\n",
    "for score_str in results[\"Score\"]:\n",
    "    values = [float(x) for x in score_str.split(\",\")]\n",
    "    for v in values:\n",
    "        s += v\n",
    "        n += 1\n",
    "    running_avg.append(s / n)\n",
    "\n",
    "results[\"RunningAvgScore\"] = running_avg\n",
    "\n",
    "# plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(results[\"Question\"], results[\"RunningAvgScore\"], marker='o')\n",
    "plt.ylim(2.2, 2.8)\n",
    "plt.title(\"Average Score Over Questions (EV)\")\n",
    "plt.xlabel(\"Question Number\")\n",
    "plt.ylabel(\"Cumulative Mean Score\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"average_score/EV_avg_score.png\", dpi=400)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0735cc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['Question', 'Choice', 'Score'])\n",
    "score_history = {}   # {option: [scores]}\n",
    "\n",
    "# main loop\n",
    "for index, row in questions.iterrows():\n",
    "    current_options = get_current_options(row, scores, index)\n",
    "\n",
    "    # compute mean − variance for each option\n",
    "    score_list = []\n",
    "    for option, current_score in current_options.values():\n",
    "        if option in score_history and len(score_history[option]) > 1:\n",
    "            s = score_history[option]\n",
    "            mean = sum(s) / len(s)\n",
    "            std = np.sqrt(sum((x - mean) ** 2 for x in s) / len(s))\n",
    "            value = mean - std\n",
    "        else:\n",
    "            value = 0\n",
    "        score_list.append((option, value))\n",
    "\n",
    "    # select items with highest score (may be multiple)\n",
    "    best_score = max(v for _, v in score_list)\n",
    "    best_items = [opt for opt, v in score_list if v == best_score]\n",
    "\n",
    "    # update score memory\n",
    "    for option, current_score in current_options.values():\n",
    "        score_history.setdefault(option, []).append(current_score)\n",
    "\n",
    "    results = record_choice(row['Question'], best_items, current_options, results)\n",
    "\n",
    "# convert Score list to comma-form string\n",
    "results[\"Score\"] = [\n",
    "    \",\".join(str(x) for x in sc) if isinstance(sc, list) else str(sc)\n",
    "    for sc in results[\"Score\"]\n",
    "]\n",
    "\n",
    "# compute cumulative running mean score\n",
    "running_avg = []\n",
    "total, count = 0, 0\n",
    "for score_str in results[\"Score\"]:\n",
    "    vals = [float(x) for x in score_str.split(\",\")]\n",
    "    for v in vals:\n",
    "        total += v\n",
    "        count += 1\n",
    "    running_avg.append(total / count)\n",
    "\n",
    "results[\"RunningAvgScore\"] = running_avg\n",
    "\n",
    "# plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(results[\"Question\"], results[\"RunningAvgScore\"], marker='o')\n",
    "plt.ylim(2.2, 2.8)\n",
    "plt.title(\"Average Score Over Questions (RA)\")\n",
    "plt.xlabel(\"Question Number\")\n",
    "plt.ylabel(\"Cumulative Mean Score\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"average_score/RA_score.png\", dpi=400)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23f89754",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['Question', 'Choice', 'Score'])\n",
    "score_history = {}   # {option: [scores]}\n",
    "\n",
    "# main loop\n",
    "for index, row in questions.iterrows():\n",
    "    current_options = get_current_options(row, scores, index)\n",
    "\n",
    "    # compute mean − variance for each option\n",
    "    score_list = []\n",
    "    for option, current_score in current_options.values():\n",
    "        if option in score_history and len(score_history[option]) > 1:\n",
    "            s = score_history[option]\n",
    "            mean = sum(s) / len(s)\n",
    "            std = np.sqrt(sum((x + mean) ** 2 for x in s) / len(s))\n",
    "            value = mean - std\n",
    "        else:\n",
    "            value = 0\n",
    "        score_list.append((option, value))\n",
    "\n",
    "    # select items with highest score (may be multiple)\n",
    "    best_score = max(v for _, v in score_list)\n",
    "    best_items = [opt for opt, v in score_list if v == best_score]\n",
    "\n",
    "    # update score memory\n",
    "    for option, current_score in current_options.values():\n",
    "        score_history.setdefault(option, []).append(current_score)\n",
    "\n",
    "    results = record_choice(row['Question'], best_items, current_options, results)\n",
    "\n",
    "# convert Score list to comma-form string\n",
    "results[\"Score\"] = [\n",
    "    \",\".join(str(x) for x in sc) if isinstance(sc, list) else str(sc)\n",
    "    for sc in results[\"Score\"]\n",
    "]\n",
    "\n",
    "# compute cumulative running mean score\n",
    "running_avg = []\n",
    "total, count = 0, 0\n",
    "for score_str in results[\"Score\"]:\n",
    "    vals = [float(x) for x in score_str.split(\",\")]\n",
    "    for v in vals:\n",
    "        total += v\n",
    "        count += 1\n",
    "    running_avg.append(total / count)\n",
    "\n",
    "results[\"RunningAvgScore\"] = running_avg\n",
    "\n",
    "# plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(results[\"Question\"], results[\"RunningAvgScore\"], marker='o')\n",
    "plt.ylim(2.2, 2.8)\n",
    "plt.title(\"Average Score Over Questions (RS)\")\n",
    "plt.xlabel(\"Question Number\")\n",
    "plt.ylabel(\"Cumulative Mean Score\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"average_score/RS_avg_score.png\", dpi=400)\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
